{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47d4dee-ed3e-44e3-a79f-1eaff89b8d50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T17:41:52.970629Z",
     "iopub.status.busy": "2025-07-02T17:41:52.969628Z",
     "iopub.status.idle": "2025-07-02T17:42:28.538566Z",
     "shell.execute_reply": "2025-07-02T17:42:28.537252Z",
     "shell.execute_reply.started": "2025-07-02T17:41:52.970629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    concatenate_datasets,\n",
    "    load_dataset,\n",
    "    load_from_disk,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from nameparser import HumanName\n",
    "from names_dataset import NameDataset, NameWrapper\n",
    "from ethnicseer import EthnicClassifier\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import pycountry_convert as pc\n",
    "import pycountry\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, cohen_kappa_score\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from datasets import ClassLabel\n",
    "from evaluate import load as load_metric\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from itertools import combinations\n",
    "import krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5750282c-255a-4591-ac0e-73ec0323c8a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T17:42:28.542546Z",
     "iopub.status.busy": "2025-07-02T17:42:28.541548Z",
     "iopub.status.idle": "2025-07-02T17:42:28.605462Z",
     "shell.execute_reply": "2025-07-02T17:42:28.603933Z",
     "shell.execute_reply.started": "2025-07-02T17:42:28.542546Z"
    }
   },
   "outputs": [],
   "source": [
    "ontonotes_nw_encoded = load_from_disk(\"splits/ontonotes_newswire\")\n",
    "ontonotes_rest_encoded = load_from_disk(\"splits/ontonotes_rest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682aedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_list):\n",
    "\n",
    "    def process_single(data):\n",
    "        word_ids = data['word_ids']\n",
    "        predictions = data['predictions']\n",
    "        gold = data['gold']\n",
    "        tokenized_tokens = data['tokens']\n",
    "\n",
    "        word_ids = [a for a in word_ids if a is not None]\n",
    "\n",
    "        processed_predictions = []\n",
    "        processed_gold = []\n",
    "\n",
    "        current_word_id = None\n",
    "        current_predictions = []\n",
    "        current_gold = []\n",
    "\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id != current_word_id:\n",
    "                if current_predictions:\n",
    "                    processed_predictions.append(\n",
    "                        Counter(current_predictions).most_common(1)[0][0])\n",
    "                    processed_gold.append(\n",
    "                        Counter(current_gold).most_common(1)[0][0])\n",
    "\n",
    "                current_word_id = word_id\n",
    "                current_predictions = [predictions[idx]]\n",
    "                current_gold = [gold[idx]]\n",
    "            else:\n",
    "                current_predictions.append(predictions[idx])\n",
    "                current_gold.append(gold[idx])\n",
    "\n",
    "        if current_predictions:\n",
    "            processed_predictions.append(\n",
    "                Counter(current_predictions).most_common(1)[0][0])\n",
    "            processed_gold.append(\n",
    "                Counter(current_gold).most_common(1)[0][0])\n",
    "\n",
    "        return processed_predictions, processed_gold\n",
    "\n",
    "    processed_predictions_list = []\n",
    "    processed_gold_list = []\n",
    "\n",
    "    for data in data_list:\n",
    "        processed_predictions, processed_gold = process_single(data)\n",
    "        processed_predictions_list.append(processed_predictions)\n",
    "        processed_gold_list.append(processed_gold)\n",
    "\n",
    "    return processed_predictions_list, processed_gold_list\n",
    "\n",
    "\n",
    "def evaluate_predictions(p, test_data):\n",
    "    predictions, labels, _ = p\n",
    "\n",
    "    pred_indices = [np.argmax(p, axis=-1) for p in predictions]\n",
    "    label_indices = labels\n",
    "\n",
    "    pred_tags = [[id2label[p] for p, l in zip(p_seq, l_seq) if l != -100]\n",
    "                 for p_seq, l_seq in zip(pred_indices, label_indices)]\n",
    "    gold_tags = [[id2label[l] for l in l_seq if l != -100]\n",
    "                 for l_seq in label_indices]\n",
    "\n",
    "    def add_preds(example, idx):\n",
    "        length = len(example['word_ids'])\n",
    "        example['predictions'] = pred_tags[idx][:length]\n",
    "        example['gold'] = gold_tags[idx][:length]\n",
    "        return example\n",
    "\n",
    "    test_data = test_data.map(add_preds, with_indices=True)\n",
    "\n",
    "    length = len(test_data['predictions'][0])\n",
    "\n",
    "    pred, gold = process_data(test_data)\n",
    "\n",
    "    flat_pred = [label for seq in pred for label in seq]\n",
    "    flat_gold = [label for seq in gold for label in seq]\n",
    "\n",
    "    print(classification_report(flat_gold, flat_pred, zero_division=0))\n",
    "\n",
    "    return (flat_pred, flat_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e96baf-9152-4eae-a886-4e8402f779df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T17:42:28.609461Z",
     "iopub.status.busy": "2025-07-02T17:42:28.608466Z",
     "iopub.status.idle": "2025-07-02T17:42:29.274651Z",
     "shell.execute_reply": "2025-07-02T17:42:29.273269Z",
     "shell.execute_reply.started": "2025-07-02T17:42:28.609461Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "id2label = {\n",
    "    0: \"O\", 1: \"B-PER\", 2: \"I-PER\", 3: \"B-ORG\", 4: \"I-ORG\",\n",
    "    5: \"B-LOC\", 6: \"I-LOC\", 7: \"B-MISC\", 8: \"I-MISC\"\n",
    "}\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        true_predictions.append(\n",
    "            [id2label[p] for p, l in zip(pred_seq, label_seq) if l != -100])\n",
    "        true_labels.append([id2label[l] for l in label_seq if l != -100])\n",
    "\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b36791c-e293-4601-9c09-de06aa830c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T17:43:04.168901Z",
     "iopub.status.busy": "2025-07-02T17:43:04.167898Z",
     "iopub.status.idle": "2025-07-02T17:43:04.179425Z",
     "shell.execute_reply": "2025-07-02T17:43:04.177901Z",
     "shell.execute_reply.started": "2025-07-02T17:43:04.168901Z"
    }
   },
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-PER', 'I-PER', 'B-ORG',\n",
    "              'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=True,\n",
    "        return_special_tokens_mask=True,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "    all_word_ids = []\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        all_word_ids.append(word_ids)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(labels[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    tokenized_inputs[\"word_ids\"] = all_word_ids\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c51dc4d6-a037-4beb-87ed-915efc21b015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T17:43:04.762256Z",
     "iopub.status.busy": "2025-07-02T17:43:04.761017Z",
     "iopub.status.idle": "2025-07-02T17:43:08.428867Z",
     "shell.execute_reply": "2025-07-02T17:43:08.426855Z",
     "shell.execute_reply.started": "2025-07-02T17:43:04.762256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb697302f47492aab2efebeb0cb0484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100516 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ['cased', 'uncased']:\n",
    "    print(i)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        \"./saved_model/bert-base-\"+i+\"_conll\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-'+i)\n",
    "\n",
    "    ontonotes_nw_encoded = ontonotes_nw_encoded.map(\n",
    "        tokenize_and_align_labels, batched=True)\n",
    "    ontonotes_rest_encoded = ontonotes_rest_encoded.map(\n",
    "        tokenize_and_align_labels, batched=True)\n",
    "\n",
    "    conll_args = TrainingArguments(output_dir=\"./results/random\",\n",
    "                                   per_device_eval_batch_size=8, report_to=\"none\")\n",
    "\n",
    "    conll_trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "        args=conll_args,\n",
    "    )\n",
    "    print('news')\n",
    "    base_conll = conll_trainer.predict(ontonotes_nw_encoded)\n",
    "    x, y = evaluate_predictions(base_conll, ontonotes_nw_encoded)\n",
    "\n",
    "    print('rest')\n",
    "    base_conll = conll_trainer.predict(ontonotes_rest_encoded)\n",
    "    x, y = evaluate_predictions(base_conll, ontonotes_rest_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02edcd1d-9768-44b1-9d11-95f854191dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
