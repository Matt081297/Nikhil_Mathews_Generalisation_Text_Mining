{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c15dd3-5458-4883-9e72-a7ab6ff1db94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:33:55.046162Z",
     "iopub.status.busy": "2025-07-03T16:33:55.045788Z",
     "iopub.status.idle": "2025-07-03T16:35:11.461249Z",
     "shell.execute_reply": "2025-07-03T16:35:11.460702Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scistor/guest/lre040/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    concatenate_datasets,\n",
    "    load_dataset,\n",
    "    load_from_disk,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from nameparser import HumanName\n",
    "from names_dataset import NameDataset, NameWrapper\n",
    "from ethnicseer import EthnicClassifier\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import pycountry_convert as pc\n",
    "import pycountry\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, cohen_kappa_score\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from datasets import ClassLabel\n",
    "from evaluate import load as load_metric\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from itertools import combinations\n",
    "import krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ecc9f10-5de7-4101-b978-d5a8c0cccdba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:35:11.463640Z",
     "iopub.status.busy": "2025-07-03T16:35:11.463202Z",
     "iopub.status.idle": "2025-07-03T16:35:11.466950Z",
     "shell.execute_reply": "2025-07-03T16:35:11.466492Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503ac47c-b69d-47fd-bd74-30b26ba97c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:35:11.468742Z",
     "iopub.status.busy": "2025-07-03T16:35:11.468486Z",
     "iopub.status.idle": "2025-07-03T16:35:11.666162Z",
     "shell.execute_reply": "2025-07-03T16:35:11.665697Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conll_main = load_from_disk(\"./splits/conll_main\")\n",
    "conll_clean = load_from_disk(\"./splits/conll_clean\")\n",
    "\n",
    "ontonotes_main = load_from_disk(\"./splits/ontonotes_main\")\n",
    "ontonotes_clean = load_from_disk(\"./splits/ontonotes_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b08f4ae-7b2d-448a-8217-dbc8acb0e845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:35:11.668173Z",
     "iopub.status.busy": "2025-07-03T16:35:11.667909Z",
     "iopub.status.idle": "2025-07-03T16:35:14.058592Z",
     "shell.execute_reply": "2025-07-03T16:35:14.057988Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "flat_conll = list(chain.from_iterable(conll_main['tokens']))\n",
    "flat_onto = list(chain.from_iterable(ontonotes_main['tokens']))\n",
    "flat_conll_clean = list(chain.from_iterable(conll_clean['tokens']))\n",
    "flat_onto_clean = list(chain.from_iterable(ontonotes_clean['tokens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5369020-ce17-44a1-b634-25d6e770ccfe",
   "metadata": {},
   "source": [
    "# Load GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd834e98-1132-4ea7-ab80-12501a446f6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:35:14.061015Z",
     "iopub.status.busy": "2025-07-03T16:35:14.060685Z",
     "iopub.status.idle": "2025-07-03T16:35:14.201117Z",
     "shell.execute_reply": "2025-07-03T16:35:14.200367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d661b808-b12c-4bd9-9e89-606cf2b1e4f7",
   "metadata": {},
   "source": [
    "# Tokenisation & Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831cedee-f0f0-4c67-b225-4b6af76759b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:35:14.203227Z",
     "iopub.status.busy": "2025-07-03T16:35:14.202821Z",
     "iopub.status.idle": "2025-07-03T16:35:14.207767Z",
     "shell.execute_reply": "2025-07-03T16:35:14.207350Z"
    }
   },
   "outputs": [],
   "source": [
    "ontonotes_id_to_label = {\n",
    "    0: \"O\", 1: \"B-CARDINAL\", 2: \"B-DATE\", 3: \"I-DATE\", 4: \"B-PERSON\", 5: \"I-PERSON\",\n",
    "    6: \"B-NORP\", 7: \"B-GPE\", 8: \"I-GPE\", 9: \"B-LAW\", 10: \"I-LAW\", 11: \"B-ORG\", 12: \"I-ORG\",\n",
    "    13: \"B-PERCENT\", 14: \"I-PERCENT\", 15: \"B-ORDINAL\", 16: \"B-MONEY\", 17: \"I-MONEY\",\n",
    "    18: \"B-WORK_OF_ART\", 19: \"I-WORK_OF_ART\", 20: \"B-FAC\", 21: \"B-TIME\", 22: \"I-CARDINAL\",\n",
    "    23: \"B-LOC\", 24: \"B-QUANTITY\", 25: \"I-QUANTITY\", 26: \"I-NORP\", 27: \"I-LOC\",\n",
    "    28: \"B-PRODUCT\", 29: \"I-TIME\", 30: \"B-EVENT\", 31: \"I-EVENT\", 32: \"I-FAC\",\n",
    "    33: \"B-LANGUAGE\", 34: \"I-PRODUCT\", 35: \"I-ORDINAL\", 36: \"I-LANGUAGE\"\n",
    "}\n",
    "\n",
    "conll_label_to_id = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3,\n",
    "                     'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "id2label = {v: k for k, v in conll_label_to_id.items()}\n",
    "\n",
    "ontonotes_to_conll_entity = {\n",
    "    \"PERSON\": \"PER\", \"ORG\": \"ORG\", \"GPE\": \"LOC\", \"LOC\": \"LOC\",\n",
    "    \"NORP\": \"MISC\", \"FAC\": \"MISC\", \"EVENT\": \"MISC\", \"WORK_OF_ART\": \"MISC\",\n",
    "    \"LAW\": \"MISC\", \"PRODUCT\": \"MISC\", \"LANGUAGE\": \"MISC\",\n",
    "    \"DATE\": None, \"TIME\": None, \"PERCENT\": None, \"MONEY\": None,\n",
    "    \"QUANTITY\": None, \"ORDINAL\": None, \"CARDINAL\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d151cb53-9464-4c7e-aa46-a65b0e7d5fcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:35:14.209419Z",
     "iopub.status.busy": "2025-07-03T16:35:14.209181Z",
     "iopub.status.idle": "2025-07-03T16:35:14.216569Z",
     "shell.execute_reply": "2025-07-03T16:35:14.216077Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(data_list):\n",
    "\n",
    "    def process_single(data):\n",
    "        word_ids = data['word_ids']\n",
    "        predictions = data['predictions']\n",
    "        gold = data['gold']\n",
    "        tokenized_tokens = data['tokens']\n",
    "\n",
    "        word_ids = [a for a in word_ids if a is not None]\n",
    "\n",
    "        processed_predictions = []\n",
    "        processed_gold = []\n",
    "\n",
    "        current_word_id = None\n",
    "        current_predictions = []\n",
    "        current_gold = []\n",
    "\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id != current_word_id:\n",
    "                if current_predictions:\n",
    "                    processed_predictions.append(\n",
    "                        Counter(current_predictions).most_common(1)[0][0])\n",
    "                    processed_gold.append(\n",
    "                        Counter(current_gold).most_common(1)[0][0])\n",
    "\n",
    "                current_word_id = word_id\n",
    "                current_predictions = [predictions[idx]]\n",
    "                current_gold = [gold[idx]]\n",
    "            else:\n",
    "                current_predictions.append(predictions[idx])\n",
    "                current_gold.append(gold[idx])\n",
    "\n",
    "        if current_predictions:\n",
    "            processed_predictions.append(\n",
    "                Counter(current_predictions).most_common(1)[0][0])\n",
    "            processed_gold.append(\n",
    "                Counter(current_gold).most_common(1)[0][0])\n",
    "\n",
    "        return processed_predictions, processed_gold\n",
    "\n",
    "    processed_predictions_list = []\n",
    "    processed_gold_list = []\n",
    "\n",
    "    for data in data_list:\n",
    "        processed_predictions, processed_gold = process_single(data)\n",
    "        processed_predictions_list.append(processed_predictions)\n",
    "        processed_gold_list.append(processed_gold)\n",
    "\n",
    "    return processed_predictions_list, processed_gold_list\n",
    "\n",
    "\n",
    "def evaluate_predictions(p, test_data):\n",
    "    predictions, labels, _ = p\n",
    "\n",
    "    pred_indices = [np.argmax(p, axis=-1) for p in predictions]\n",
    "    label_indices = labels\n",
    "\n",
    "    pred_tags = [[id2label[p] for p, l in zip(p_seq, l_seq) if l != -100]\n",
    "                 for p_seq, l_seq in zip(pred_indices, label_indices)]\n",
    "    gold_tags = [[id2label[l] for l in l_seq if l != -100]\n",
    "                 for l_seq in label_indices]\n",
    "\n",
    "    def add_preds(example, idx):\n",
    "        length = len(example['word_ids'])\n",
    "        example['predictions'] = pred_tags[idx][:length]\n",
    "        example['gold'] = gold_tags[idx][:length]\n",
    "        return example\n",
    "\n",
    "    test_data = test_data.map(add_preds, with_indices=True)\n",
    "\n",
    "    length = len(test_data['predictions'][0])\n",
    "\n",
    "    pred, gold = process_data(test_data)\n",
    "\n",
    "    flat_pred = [label for seq in pred for label in seq]\n",
    "    flat_gold = [label for seq in gold for label in seq]\n",
    "\n",
    "    print(classification_report(flat_gold, flat_pred, zero_division=0))\n",
    "\n",
    "    return (flat_pred, flat_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f3daecd-13ac-46f9-8714-d1b09add4691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:35:14.218181Z",
     "iopub.status.busy": "2025-07-03T16:35:14.217995Z",
     "iopub.status.idle": "2025-07-03T16:35:14.659669Z",
     "shell.execute_reply": "2025-07-03T16:35:14.659142Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "label_list = ['O', 'B-PER', 'I-PER', 'B-ORG',\n",
    "              'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=True,\n",
    "        return_special_tokens_mask=True,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "    all_word_ids = []\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        all_word_ids.append(word_ids)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(labels[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    tokenized_inputs[\"word_ids\"] = all_word_ids\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c54715b-4936-44d7-8bbb-8de8b99c9e4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:35:14.662050Z",
     "iopub.status.busy": "2025-07-03T16:35:14.661674Z",
     "iopub.status.idle": "2025-07-03T16:35:14.742749Z",
     "shell.execute_reply": "2025-07-03T16:35:14.742191Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conll_main = conll_main.map(tokenize_and_align_labels, batched=True)\n",
    "conll_clean = conll_clean.map(tokenize_and_align_labels, batched=True)\n",
    "ontonotes_main = ontonotes_main.map(tokenize_and_align_labels, batched=True)\n",
    "ontonotes_clean = ontonotes_clean.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4231967-66a6-40a3-82ad-6be8ff04fcae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:35:14.744922Z",
     "iopub.status.busy": "2025-07-03T16:35:14.744627Z",
     "iopub.status.idle": "2025-07-03T16:35:14.748372Z",
     "shell.execute_reply": "2025-07-03T16:35:14.747969Z"
    }
   },
   "outputs": [],
   "source": [
    "def caps_correct(flat_tokens, x, y):\n",
    "    if len(flat_tokens) == len(x) == len(y):\n",
    "\n",
    "        correct_caps = 0\n",
    "        incorrect_caps = 0\n",
    "\n",
    "        for token, pred, gold in zip(flat_tokens, x, y):\n",
    "            if token and token[0].isupper():\n",
    "                if pred == gold:\n",
    "                    correct_caps += 1\n",
    "                else:\n",
    "                    if incorrect_caps<30:\n",
    "                        print(token , pred , gold)\n",
    "                    incorrect_caps += 1\n",
    "\n",
    "        print(f\"Capitalized Tokens - Correct Predictions: {correct_caps}\")\n",
    "        print(f\"Capitalized Tokens - Incorrect Predictions: {incorrect_caps}\")\n",
    "    else:\n",
    "        print('not equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9146a939-18e4-449b-822e-bd0eb1c93be8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:35:14.750192Z",
     "iopub.status.busy": "2025-07-03T16:35:14.749798Z",
     "iopub.status.idle": "2025-07-03T16:35:15.494573Z",
     "shell.execute_reply": "2025-07-03T16:35:15.494017Z"
    }
   },
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af2b6d1e-49a5-419c-8320-8e3ce35ee85e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:35:15.497174Z",
     "iopub.status.busy": "2025-07-03T16:35:15.496765Z",
     "iopub.status.idle": "2025-07-03T16:36:15.323550Z",
     "shell.execute_reply": "2025-07-03T16:36:15.322910Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/1063622/ipykernel_3801885/2181452677.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.80      0.80      0.80      8535\n",
      "      B-MISC       0.76      0.68      0.71      4062\n",
      "       B-ORG       0.75      0.50      0.60      7398\n",
      "       B-PER       0.86      0.93      0.89      7975\n",
      "       I-LOC       0.68      0.62      0.65      1356\n",
      "      I-MISC       0.53      0.58      0.56      1380\n",
      "       I-ORG       0.73      0.76      0.74      4251\n",
      "       I-PER       0.89      0.99      0.94      5503\n",
      "           O       0.98      0.99      0.99    201398\n",
      "\n",
      "    accuracy                           0.95    241858\n",
      "   macro avg       0.78      0.76      0.77    241858\n",
      "weighted avg       0.95      0.95      0.95    241858\n",
      "\n",
      "Hong B-LOC B-MISC\n",
      "Kong-based O I-MISC\n",
      "Wuxi I-ORG B-ORG\n",
      "Jiangsu I-LOC B-LOC\n",
      "ISLAMABAD O B-LOC\n",
      "BARCELONA B-MISC B-ORG\n",
      "ATLETICO O B-ORG\n",
      "SUPERCUP O B-MISC\n",
      "Portsmouth B-LOC B-ORG\n",
      "Corser O B-PER\n",
      "Vatican B-ORG B-LOC\n",
      "Owen-Jones I-PER B-PER\n",
      "Air O B-ORG\n",
      "Cargo O I-ORG\n",
      "Newsroom O I-ORG\n",
      "The B-ORG O\n",
      "Lebanese I-ORG B-ORG\n",
      "Bulatka B-LOC B-ORG\n",
      "Mapei B-LOC B-ORG\n",
      "Rice B-ORG O\n",
      "RTRS O B-ORG\n",
      "NEW O B-ORG\n",
      "YORK O I-ORG\n",
      "Strasbourg B-LOC B-ORG\n",
      "World B-ORG O\n",
      "Balkan B-MISC B-LOC\n",
      "Port B-MISC B-ORG\n",
      "Christine I-MISC I-ORG\n",
      "Stentex O B-ORG\n",
      "Groningen B-LOC B-ORG\n",
      "Capitalized Tokens - Correct Predictions: 43648\n",
      "Capitalized Tokens - Incorrect Predictions: 9832\n"
     ]
    }
   ],
   "source": [
    "train_data = ontonotes_main\n",
    "test_data = conll_main\n",
    "\n",
    "train_data_name = 'onto'\n",
    "\n",
    "model_path = f\"./saved_model/{model_name}_{train_data_name}\"\n",
    "mod = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./output/random\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=mod,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(test_data)\n",
    "\n",
    "x, y = evaluate_predictions(predictions, test_data)\n",
    "\n",
    "caps_correct(flat_conll, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08676971-9cdd-40ae-b790-8304c8054b5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:36:15.325521Z",
     "iopub.status.busy": "2025-07-03T16:36:15.325256Z",
     "iopub.status.idle": "2025-07-03T16:40:18.328895Z",
     "shell.execute_reply": "2025-07-03T16:40:18.328190Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/1063622/ipykernel_3801885/3412323054.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.83      0.89      0.86     17495\n",
      "      B-MISC       0.64      0.73      0.68     10657\n",
      "       B-ORG       0.63      0.68      0.65     13041\n",
      "       B-PER       0.90      0.93      0.91     15547\n",
      "       I-LOC       0.71      0.66      0.68      5367\n",
      "      I-MISC       0.66      0.37      0.47      7305\n",
      "       I-ORG       0.90      0.71      0.79     18313\n",
      "       I-PER       0.95      0.88      0.91     11086\n",
      "           O       0.99      0.99      0.99   1011383\n",
      "\n",
      "    accuracy                           0.97   1110194\n",
      "   macro avg       0.80      0.76      0.77   1110194\n",
      "weighted avg       0.97      0.97      0.97   1110194\n",
      "\n",
      "Iraq B-LOC I-MISC\n",
      "A B-LOC I-LOC\n",
      "Happy B-MISC I-MISC\n",
      "D. B-ORG B-MISC\n",
      "Wash. B-ORG B-LOC\n",
      "Capital B-ORG I-ORG\n",
      "Smirnoff B-MISC B-ORG\n",
      "Jack B-PER B-ORG\n",
      "Daniel I-PER I-ORG\n",
      "NYSE B-MISC B-ORG\n",
      "White B-LOC I-ORG\n",
      "House I-LOC I-ORG\n",
      "Kate O B-PER\n",
      "Mr. B-MISC O\n",
      "October B-MISC B-PER\n",
      "Grinch O B-PER\n",
      "A. B-PER B-ORG\n",
      "Schulman I-PER I-ORG\n",
      "Mason I-PER B-PER\n",
      "United B-ORG I-MISC\n",
      "Nations I-ORG I-MISC\n",
      "Ministry B-ORG I-ORG\n",
      "Air B-ORG B-MISC\n",
      "Force I-ORG I-MISC\n",
      "One I-ORG I-MISC\n",
      "Zhejiang B-LOC B-PER\n",
      "Cane O B-ORG\n",
      "White B-ORG O\n",
      "Paper I-ORG O\n",
      "Wuerttemburg B-LOC I-LOC\n",
      "Capitalized Tokens - Correct Predictions: 132314\n",
      "Capitalized Tokens - Incorrect Predictions: 18339\n"
     ]
    }
   ],
   "source": [
    "train_data = conll_main\n",
    "test_data = ontonotes_main\n",
    "\n",
    "train_data_name = 'conll'\n",
    "\n",
    "model_path = f\"./saved_model/{model_name}_{train_data_name}\"\n",
    "mod = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./output/random\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=mod,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(test_data)\n",
    "\n",
    "x, y = evaluate_predictions(predictions, test_data)\n",
    "\n",
    "caps_correct(flat_onto, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "537449c4-9fd3-481c-9fe0-ac9aba5e1365",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:40:18.331001Z",
     "iopub.status.busy": "2025-07-03T16:40:18.330670Z",
     "iopub.status.idle": "2025-07-03T16:40:33.675580Z",
     "shell.execute_reply": "2025-07-03T16:40:33.674786Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/1063622/ipykernel_3801885/4249636180.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.96      0.97      0.96      2110\n",
      "      B-MISC       0.92      0.91      0.92      1000\n",
      "       B-ORG       0.94      0.95      0.95      1925\n",
      "       B-PER       0.98      0.98      0.98      2084\n",
      "       I-LOC       0.91      0.90      0.91       315\n",
      "      I-MISC       0.86      0.85      0.86       337\n",
      "       I-ORG       0.93      0.96      0.94      1039\n",
      "       I-PER       1.00      0.99      0.99      1488\n",
      "           O       1.00      1.00      1.00     49262\n",
      "\n",
      "    accuracy                           0.99     59560\n",
      "   macro avg       0.95      0.95      0.95     59560\n",
      "weighted avg       0.99      0.99      0.99     59560\n",
      "\n",
      "MRI O B-MISC\n",
      "Santander B-ORG B-LOC\n",
      "BAYERN B-PER B-ORG\n",
      "BUNDESLIGA B-LOC B-MISC\n",
      "Nicol B-ORG B-PER\n",
      "Eyles B-ORG B-PER\n",
      "Leicester B-ORG B-LOC\n",
      "New B-MISC O\n",
      "AHOLD B-LOC B-ORG\n",
      "Eurograde B-MISC O\n",
      "Far B-LOC O\n",
      "North I-LOC O\n",
      "Czech B-LOC B-ORG\n",
      "Maronite B-MISC B-ORG\n",
      "UK-US B-LOC B-MISC\n",
      "BTPs B-ORG O\n",
      "CAMPESE B-LOC B-PER\n",
      "Mr B-MISC B-PER\n",
      "Clean I-MISC I-PER\n",
      "World B-MISC I-MISC\n",
      "European B-MISC I-MISC\n",
      "Battle I-MISC I-ORG\n",
      "Cry I-MISC I-ORG\n",
      "Burj B-ORG B-LOC\n",
      "Society I-ORG I-LOC\n",
      "China I-LOC B-LOC\n",
      "Kenda B-ORG B-MISC\n",
      "IRISH B-MISC B-ORG\n",
      "INDEPENDENT I-MISC I-ORG\n",
      "Zambian B-MISC O\n",
      "Capitalized Tokens - Correct Predictions: 13100\n",
      "Capitalized Tokens - Incorrect Predictions: 511\n"
     ]
    }
   ],
   "source": [
    "train_data = conll_main\n",
    "test_data = conll_clean\n",
    "\n",
    "train_data_name = 'conll'\n",
    "\n",
    "model_path = f\"./saved_model/{model_name}_{train_data_name}\"\n",
    "mod = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./output/random\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=mod,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(test_data)\n",
    "\n",
    "x, y = evaluate_predictions(predictions, test_data)\n",
    "\n",
    "caps_correct(flat_conll_clean, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a559f97e-a334-435f-91b0-d1cf5058f694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:40:33.677488Z",
     "iopub.status.busy": "2025-07-03T16:40:33.677264Z",
     "iopub.status.idle": "2025-07-03T16:41:30.646320Z",
     "shell.execute_reply": "2025-07-03T16:41:30.645839Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/1063622/ipykernel_3801885/916289436.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.97      0.97      0.97      4315\n",
      "      B-MISC       0.91      0.91      0.91      2722\n",
      "       B-ORG       0.92      0.94      0.93      3314\n",
      "       B-PER       0.96      0.97      0.96      3890\n",
      "       I-LOC       0.93      0.92      0.93      1258\n",
      "      I-MISC       0.87      0.84      0.85      2067\n",
      "       I-ORG       0.94      0.96      0.95      4675\n",
      "       I-PER       0.97      0.97      0.97      2868\n",
      "           O       1.00      1.00      1.00    253580\n",
      "\n",
      "    accuracy                           0.99    278689\n",
      "   macro avg       0.94      0.94      0.94    278689\n",
      "weighted avg       0.99      0.99      0.99    278689\n",
      "\n",
      "Constitutional O B-MISC\n",
      "CCTV O B-ORG\n",
      "Albo B-ORG B-PER\n",
      "THE B-ORG O\n",
      "CHILDREN B-ORG O\n",
      "Child O B-MISC\n",
      "Game O I-MISC\n",
      "I O B-PER\n",
      "Arab B-MISC I-PER\n",
      "Contra B-MISC B-ORG\n",
      "Pan B-ORG B-PER\n",
      "American I-ORG I-PER\n",
      "Thanksgiving B-MISC O\n",
      "Iowa B-LOC I-LOC\n",
      "WW2 B-ORG B-MISC\n",
      "PhD B-MISC O\n",
      "Suez I-MISC O\n",
      "Canal I-MISC O\n",
      "Arab I-MISC B-MISC\n",
      "Western B-MISC O\n",
      "Mercedes B-MISC B-ORG\n",
      "Africa B-LOC I-LOC\n",
      "RCI O B-ORG\n",
      "YLE O B-ORG\n",
      "Finish O B-MISC\n",
      "Polo B-ORG B-MISC\n",
      "Mom O B-PER\n",
      "Agora B-MISC B-ORG\n",
      "Garden I-MISC I-ORG\n",
      "Shuqin B-PER I-PER\n",
      "Capitalized Tokens - Correct Predictions: 36407\n",
      "Capitalized Tokens - Incorrect Predictions: 1398\n"
     ]
    }
   ],
   "source": [
    "train_data = ontonotes_main\n",
    "test_data = ontonotes_clean\n",
    "\n",
    "train_data_name = 'onto'\n",
    "\n",
    "model_path = f\"./saved_model/{model_name}_{train_data_name}\"\n",
    "mod = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./output/random\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=mod,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(test_data)\n",
    "\n",
    "x, y = evaluate_predictions(predictions, test_data)\n",
    "\n",
    "caps_correct(flat_onto_clean, x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
